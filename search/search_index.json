{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RAGLib - a comprehensive collection of RAG techniques.","text":"<p>Many Modern RAG Techniques in One Place \u2014 Explained, Compared, and Ready to Use</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Retrieval-Augmented Generation (RAG) is transforming the landscape of generative AI by seamlessly integrating information retrieval with advanced generation capabilities. This repository presents a carefully curated collection of modern techniques designed to enhance the performance of RAG systems, enabling them to produce responses that are more accurate, context-aware, and informative.</p>"},{"location":"#rag-techniques","title":"RAG Techniques","text":"<p>Explore the comprehensive list of Retrieval-Augmented Generation methods that have been considered for this library.</p> # Category Technique View 1 Foundational \ud83e\uddf1 Vector RAG 2 Advanced Architecture \ud83d\udef0\ufe0f GNN-RAG - in progress 3 Advanced Architecture \ud83d\udef0\ufe0f T-RAG - in progress 4 Advanced Architecture \ud83d\udef0\ufe0f Microsoft GraphRAG Variants 5 Advanced Architecture \ud83d\udef0\ufe0f Microsoft GraphRAG - in progress 5 Advanced Architecture \ud83d\udef0\ufe0f LightRAG - in progress 5 Advanced Architecture \ud83d\udef0\ufe0f PathRAG - in progress 2 Advanced Retrieval \ud83e\udde0 3 Iterative Techniques \ud83d\udd01 4 Context Enrichment \ud83c\udf10\ud83e\udde9 5 Evaluation \ud83d\udcca 6 Query Enhancement \ud83d\udd0d\u2728\ud83d\udd2d"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#requirements","title":"Requirements","text":"Name Installation Purpose Python ^3.10 Download The library is Python-based."},{"location":"getting-started/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip3 install -r requirements.txt\n</code></pre>"},{"location":"getting-started/#set-up-your-workspace-variables","title":"Set Up Your Workspace Variables","text":"<ul> <li><code>config.ini</code> contains the environment variables and settings required to run the pipeline. You can modify this file to change the settings for the pipeline.</li> </ul>"},{"location":"getting-started/#quickstart-guide","title":"Quickstart Guide","text":"<p>If you just want to get started quickly, follow the steps provided here. When wanting to dive deeper, read Initialization instead.</p> <ol> <li>Download the repo onto a machine running docker</li> <li> <p>Configure the following settings in the config file:</p> <ul> <li>Point <code>[general] data_dir</code> to your data or move the data into a subfolder <code>/dir</code> of this folder</li> <li>Change <code>[arangodb] url</code> to use the ip of your machine. A docker container will start the database on port <code>13401</code></li> <li>Change <code>[database] host</code> to the ip of your machine. A docker container will start a postgres db on port <code>13404</code></li> <li>Change <code>[elastic] url</code> to use the ip of your machine. A docker container will start elastic db on port <code>13403</code></li> <li>Change <code>[LLM] base_url</code> to an Ollama endpoint of your choice. This service will not be started when running this setup. For small datasets, you might use <code>https://ollama.nimbus.dlr.de/ollama</code> (the url of the DLR-intern Ollama nimbus server)</li> <li>If needed, insert an api_key into <code>[LLM] static_api_key</code>. When using the nimbus server, this is required</li> </ul> </li> <li> <p>Make sure, that the specified Ollama service runs the model <code>llama3.2:latest</code> (recommended) or switch to another model by changing <code>[LLM] model_name</code></p> </li> <li>Provide file read and write rights to the database data directory, so the docker containers can create needed files. This can be done with <code>chmod a+rw db/* -R</code></li> <li>Run <code>./run.sh init</code> in the current directory. This starts the data initialization and may take multiple days. To check the status of the initialization, read the inizialization log, created on startup in the resources folder</li> <li>Once the initialization is completed - check the initialization log for a <code>&gt;&gt;&gt; Initialization completed!</code> message at the end - shutdown any remaining docker resources by running <code>./run.sh down</code></li> </ol>"},{"location":"getting-started/#initialization-with-docker","title":"Initialization with docker","text":"<p>Before running the Initialization, follow these steps to make sure, that everything is setup:</p> <ol> <li>Make sure the data is in the right format:<ul> <li>This project requires a single folder as an input for the data</li> <li>This folder may contain any number of files or folders</li> <li>Each folder may include multiple subfolders, zip archives and documents</li> <li>For a list of valid file types look at valid file types</li> </ul> </li> <li>Check the docker setup to follow your guidelines:<ul> <li>Check all dockerfiles to follow your local naming conventions</li> <li>Check the ports in the db compose file</li> </ul> </li> <li>Check your Ollama provider:<ul> <li>Either access an already existing ollama service, or create a new one to be used for this project</li> <li>Make sure, the Ollama instance is running</li> <li>We recommend the model <code>llama3.2:latest</code>. See the documentation for installation steps.</li> </ul> </li> <li>Check the config file:<ul> <li>Point <code>[general] data_dir</code> to your data or move the data into a subfolder <code>/dir</code> of this folder</li> <li>Change <code>[arangodb] url</code> to the correct url. The default port is <code>13401</code></li> <li>Change <code>[elastic] url</code> to use the ip of your machine. The default port is <code>13403</code></li> <li>Change <code>[LLM] base_url</code> to the correct url of the provider. See the LLM section for more information</li> <li>Change <code>[LLM] model_name</code> to the correct model name. For <code>Llama3.2</code> this defaults to <code>llama3.2:latest</code></li> <li>When using an Ollama instance with an API key access restriction, input your key into <code>[LLM] static_api_key</code></li> </ul> </li> <li>Provide file read and write rights to the database data directory, so the docker containers can create needed files. This can be done with <code>chmod a+rw db/* -R</code></li> <li>Run the setup script:<ul> <li>Run <code>./run.sh init</code>. This script might take multiple days to complete. Just be patient</li> <li>Any errors will be printed into the initialization log file</li> <li>After completion, run <code>./run.sh down</code> to remove any leftover docker resources</li> </ul> </li> </ol> <p>An in depth description of all values in the config file is provided in the section Config.ini.</p>"},{"location":"getting-started/#initialization-without-docker","title":"Initialization without docker","text":"<p>Before running the Initialization, follow these steps to make sure, that everything is setup:</p> <ol> <li>Make sure the data is in the right format:<ul> <li>This project requires a single folder as an input for the data</li> <li>This folder may contain any number of files or folders</li> <li>Each folder may include multiple subfolders, zip archives and documents</li> <li>For a list of valid file types look at valid file types</li> </ul> </li> <li>Start database services:<ul> <li>Start an ArangoDB Instance</li> <li>Start an ElasticDB Instance </li> </ul> </li> <li>Check your Ollama provider:<ul> <li>Either access an already existing ollama service, or create a new one to be used for this project</li> <li>Make sure, the Ollama instance is running</li> <li>We recommend the model <code>llama3.2:latest</code>. See the documentation for installation steps.</li> </ul> </li> <li>Check the config file:<ul> <li>Point <code>[general] data_dir</code> to your data or move the data into a subfolder <code>/dir</code> of this folder</li> <li>Change <code>[arangodb] url</code> to the correct url</li> <li>Change <code>[arangodb] db_name</code> to a fitting name. This db will store the provided data in various formats</li> <li>Change <code>[elastic] url</code> to use the ip of your machine</li> <li>Change <code>[elastic] RAG_index_name &amp; GARAG_index_name</code> to a fitting name. These indices will be used for RAG and GARAG retrieval</li> <li>Change <code>[LLM] url</code> to the correct url of the provider. See the LLM section for more information</li> <li>Change <code>[LLM] model_name</code> to the correct model name. For <code>Llama3.2</code> this defaults to <code>llama3.2:latest</code></li> <li>When using an Ollama instance with an API key access restriction, input your key into <code>[LLM] static_api_key</code></li> </ul> </li> <li>Install the required python dependencies:<ul> <li>Setup a new conda environment with <code>python &gt;= 3.10</code></li> <li>Install the dependencies, listed in requirements.txt</li> </ul> </li> <li>Run the setup script:<ul> <li>Run <code>python Quickstart.py</code></li> <li>This script might take multiple days to complete. Just be patient</li> <li>Any errors will be printed into the initialization log file</li> </ul> </li> </ol> <p>An in depth description of all values in the config file is provided in the section Config.ini.</p>"},{"location":"getting-started/#provided-retrieval-implementations","title":"Provided retrieval implementations","text":""},{"location":"getting-started/#rag","title":"RAG","text":"<p>A naive rag implementation. All files are read and their content is split into chunks, preserving chapters where possible. Their embeddings are then stored into an Elasticsearch database, which will be queried for every user prompt.</p>"},{"location":"getting-started/#graph-rag","title":"GRAPH RAG","text":"<p>An implementation inspired by Graph RAG by Microsoft. The data is read and transformed into a knowledge graph, stored in ArangoDB. The resulting nodes are then grouped by their topic and summarized resulting in thematic subgraphs. During a query, these thematic summaries and the user prompt are then passed to a LLM, which generates partial answers on the information provided and a confidence value, stating the helpfulness of the provided answer. These partial answers are then ranked by their confidence and the best results are returned to the user.</p> <p>This algorithm takes an extra argument, the community level to search on. There, a level of 0 describes the usage of a single node, capturing the entire information corpus in a small description. Higher values yield more nodes, therefore providing a more precise description on multiple topics. The maximum level can only be inferred by a manual lookup in the GraphDB used. Higher values always also supply the descriptions of all nodes of lower community level. A value of 1 or 2 is advisable.</p> <p>This kind of initialization takes very long, but might be worth it, as following querys are not matched by text similarity but by the topic they reside in. The generated knowledge graph is shared between Graph Rag, Graph Rag Rag, and Garag. When using this implementation, keep in mind that a longer retrieval time is expected.</p>"},{"location":"getting-started/#naive-graph-rag","title":"NAIVE GRAPH RAG","text":"<p>An implementation that reduces the query time of the Graph Rag implementation. Instead of generating partial answers using LLMs, community summaries are filtered through an embedding comparison in the Elasticsearch database. The information summaries of the most fitting communities are then returned to the user. Therefore, this approach can be seen as a naive RAG (Rapid Answer Generation) approach on community summaries. While this implementation reduces retrieval time compared to Graph Rag, precision on non-global questions is reduced.</p>"},{"location":"getting-started/#garag","title":"GARAG","text":"<p>An implementation that reduces the hallucination of the filtered information. Communities with fitting information are first found using an embedding comparison in the Elasticsearch database. The original sources (the raw data used to generate the knowledge graph) of these communities are then ranked by their influence on these summaries and the accuracy of the vector comparison of those with the user query. The top original sources are then returned to the user. Therefore, this approach can be seen as Graph-Assisted RAG (or GARAG). It returns the same kind of information that would be obtained from a normal RAG query on the original documents, using a complex, topic-based decision-making process, instead of a direct vector comparison. It is recommended to use this method, as it combines a very fast retrieval time with good precision.</p>"},{"location":"getting-started/#configini","title":"Config.ini","text":"<p>The config.ini file controlls the entire projekt. Each value is directly used by the programm. This is a list of all the values, their meaning and their default value.</p> <p>The config file itself can be found here. An example config file of a woring setup can be seen here.</p>"},{"location":"getting-started/#general","title":"general","text":"<p>General settings affecting core parts of the program - data_dir (path): The path to the folder containing the data, that will be used for the chatbot. This value has to be set by the user, when running the script KG_1_LoadData.py during initialization. <code>Default: not set</code></p> <ul> <li> <p>parallel_limit (int): The maximum amount of threads running in parallel during the program. Also represents the maximum number of threads simoultaniously waiting for a response from a large language model. <code>Default: 8</code></p> </li> <li> <p>default_rag_method (str): RAG method used if the RetrievalRequest does not specify one. Can be set to any RetrievalMethodId. <code>Default: \"GARAG#783493\"</code></p> </li> <li> <p>default_depth (int): Default depth used if the RetrievalRequest does not specify one and the RAG method requires a depth parameter. <code>Default: 1</code></p> </li> </ul>"},{"location":"getting-started/#security","title":"security","text":"<ul> <li> <p>ssl_cert_path (path): The path to the certification file for https encryption. When using http, leave this value empty. <code>Default: not set</code></p> </li> <li> <p>ssl_key_path (path): The path to the key file for https encryption. When using http, leave this value empty. <code>Default: not set</code></p> </li> </ul>"},{"location":"getting-started/#arangodb","title":"arangodb","text":"<p>General settings to access the Arango database - username  (str): The name of the user being used to manage the database. This user has to have read, write and collection and graph create access. <code>Default: not set</code></p> <ul> <li> <p>password (str): If set, this password will be used to register as the user on the ArangoDB. If <code>None</code>, the user will be asked to enter a password at the start of the program execution (Only works during initialization without docker). <code>Default: not set</code></p> </li> <li> <p>url (url): The url that will be used to access the ArangoDB. <code>Default: not set</code></p> </li> </ul>"},{"location":"getting-started/#database","title":"database","text":"<p>General settings to access the Postgres database</p> <ul> <li>username (str): Username for the database login. <code>Default: postgres</code></li> <li>password (str): Username for the database login. <code>Default: root</code></li> <li>host (hostname): Domain used for accessing the database. <code>Default: not set</code></li> <li>port (int): Port used for accessing the database. <code>Default: not set</code></li> <li>database_name (str): Name of the database. <code>Default: postgres</code></li> </ul>"},{"location":"getting-started/#elastic","title":"elastic","text":"<ul> <li>url (url): The url used to store the index data at. <code>Default: not set</code></li> </ul>"},{"location":"getting-started/#llm-indexquery","title":"LLM (index/query)","text":"<p>Settings controlling the large language model used Settings controlling the large language model used by the program..</p> <ul> <li>base_url (str): The url used to communicate with the llm.</li> <li>model_name (str): The name of the model used when communicating with an Ollama server.</li> <li>api_key (str): When provided, this api key will be used for all Ollama llm requests.</li> <li>options (dict): llm configuration</li> </ul>"},{"location":"getting-started/#valid-file-types","title":"Valid file types","text":"<p>This is a list of al file types, recognised by KG_1_LoadData.py:</p> <ul> <li>pdf</li> <li>docx</li> <li>txt</li> <li>md</li> </ul> <p>Files not included during reading:</p> <ul> <li>Files starting with ~$...: These files are usually temporary files used, while the file is open and thus don't provide any information and are ignored.</li> </ul> <p>All other file types raise a warning, which may be examined by the user afterwards in the created log file.</p>"},{"location":"getting-started/#dive-deeper","title":"Dive Deeper","text":"<ul> <li>For more details about configuring the pipeline, see the configuration documentation.</li> <li>To learn more about Initialization, refer to the Initialization documentation.</li> <li>Check out our visualization guide for a more interactive experience in debugging and exploring the knowledge graph.</li> </ul>"},{"location":"cli/","title":"CLI Reference","text":"<p>This page provides documentation for the command-line interface.</p> <p>::: mkdocs-click     :module: src.cli.main     :command: app</p>"},{"location":"config/database-selection/","title":"Neo4j","text":"<p>..</p>"},{"location":"config/database-selection/#arangodb","title":"ArangoDB","text":"<p>..</p>"},{"location":"config/database-selection/#elasticsearch","title":"Elasticsearch","text":""},{"location":"demo/","title":"RAG Technique Notebooks","text":"<p>The following is a collection of demo Jupyter notebooks showcasing various Retrieval-Augmented Generation (RAG) techniques:</p> <ul> <li>RAG Notebook</li> <li>GNN-RAG Notebook</li> <li>T-RAG Notebook</li> <li>GraphRAG Notebook</li> <li>Microsoft GraphRAG Notebook</li> </ul> <p>The test dataset for these notebooks can be found in dataset.zip.</p>"},{"location":"demo/demo/","title":"Demo","text":"<p>TEST</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nfrom dotenv import load_dotenv\n</pre> import os import sys from dotenv import load_dotenv"},{"location":"eri/","title":"External Retrieval Interface (ERI)","text":""},{"location":"eri/#about","title":"About","text":"<p>The ERI acts as an interface, transforming a data source into an accessible storage for retrieval and other llm adjacent processes. It serves as a provider of helpful information to a given query. Published in 2025 by Hecking, Sommer, and Felderer, this standard enables decentralized data storage, allowing external data to be integrated into AI workflows. To use this service, the ERI-Standard defines methods for retrieving data from the source. These can be implemented in other workflows or integrated directly into LLM processes, as done by AI Studio.</p> <p>Within this library, an ERI-Server is implemented, enabling users to configure and select specific retrieval algorithms, which are then accessible through an API.</p>"},{"location":"eri/#the-eri-standard","title":"The ERI-Standard","text":"<p>The ERI-Standard defines various requests for server-client communication. The full documentation and specification are available in the published paper and in the official server implementation repository on GitHub.</p> <p>For authentification purpose, users can fetch available methods using the <code>/auth/methods</code> path. These can the be used to retrieve a session token when fulfilling authentification at <code>/auth</code>. The provided ERI-Server implementation allows for either <code>NONE</code> or <code>TOKEN</code> authentification. <code>NONE</code> provides free access to session tokens for all users, while <code>TOKEN</code> compares incoming Bearer authentication with a database of valid API keys.</p> <p>To fetch general information about the provided data, the <code>/datasource</code> endpoint provides a set name and description of the source.</p> <p>Information about available retrieval algorithms can be accessed under the <code>/retrieval/info</code> endpoint. Here, a short description of the processes and available parameters is provided. These methods can then be used by providing a query to the <code>/retrieval</code> endpoint. This executes the selected algorithm internally, returning the selected data to the user.</p>"},{"location":"eri/#starting-the-eri-server","title":"Starting the ERI-Server","text":"<p>To start the ERI-Server, the following command can be used, executed in the raglib subdirectory: <code>uvicorn eri:app</code></p> <p>To configure the ERI-Server, read the configuration documentation.</p>"},{"location":"eri/config/","title":"Configuration","text":""},{"location":"eri/config/#general","title":"General","text":"<p>The configuration of the ERI-Server can be done using the <code>eri.ini</code> configuration file.</p> <p>This file is split into multiple sections: </p> <ul> <li>A general configuration of the ERI-Server. This section modifies the behaviour of the entire interface.</li> <li>An optional list of sections for the provided retrieval algorithm. All retrieval algorithms configured within such a section are available for the user when querying the interface</li> </ul>"},{"location":"eri/config/#configuring-the-interface","title":"Configuring the Interface","text":"<p>To configure the general interface settings, found in the <code>ERI Settings</code> section, please follow these steps closely:</p> <ol> <li>Label your data source: To enable users to easily identify what information can be retrieved using the interface, a name and description must be provided. These can be entered into the <code>data_source_name</code> and <code>data_source_description</code> fields.</li> <li>Set privacy settings: To restrict the usage of external LLMs on internal data, the type of allowed LLM providers can be set. Please enter one of <code>SELF_HOSTED | ANY</code> into the <code>allowed_provider_type</code> field. </li> <li>Configure data access: To limit the access to the ERI-Server, and thus to the information handled by such, the type of available authorization method can be set. Please enter one of <code>NONE | TOKEN</code> into the <code>authorization_methods</code> fields or, to enable multiple options, enter a JSON formatted list of the respective methods as strings instead. For more information about the available authorization methods, please refer to authorization methods.</li> <li>[Optional] Enable https: To start the Server on HTTPS, please provide the paths to a valid SSL certificate and key in the <code>ssl_cert_path</code> and <code>ssl_key_path</code> fields, respectively. The Server is started in HTTPS mode if both fields are provided; otherwise, HTTP is used instead. </li> <li>Setup the authentication database Not necessary when only using <code>NONE</code> authentication: The needed data for authentication validation is stored in a postgres database. The connection to this database has to be specified here. Please provide a JSON formatted dictionary with the following properties to the <code>postgres_connection</code> option: <code>username, password, url, database_name</code>. For more information about the database, refer to authorization methods.</li> </ol> <p>To enable selected retrieval algorithms, follow the steps specific to the desired method.</p>"},{"location":"eri/config/#garag","title":"GARAG","text":"<p>To enable GARAG as a retrieval process, please paste the following skeleton into the configuration file and follow these steps closely:</p> <pre><code>[GARAG]\nconfig = \nemb_model = \nelastic_db_url = \narango_db = \n</code></pre> <ol> <li>Enable custom configuration: To configure the retriever, please provide a JSON formatted dictionary using the properties below in the <code>config</code> field.<ul> <li><code>top_k (int; 0 &lt; x)</code>: The maximum number of hits to retrieve. May be set by an user during a retrieval request.</li> <li><code>confidence_cutoff (float; 0 &lt; x &lt; 1)</code>: A value to specify a minimum similiarity between the query and the retrieved information. May be set by an user during a retrieval request.</li> <li><code>vector_db_index_name (string)</code>: The index to be used for searching relevant information.</li> </ul> </li> <li>Specify the embedding model used: Enter the name of the embedding model used to convert requests into vectors into the <code>emb_model</code> field. This has to match the model used during the indexing phase.</li> <li>Set the Elasticsearch connection: Enter the url to the Elasticsearch server into the <code>elastic_db_url</code> field, on which the vector index can be found.</li> <li>Set the ArangoDB connection: Please specify the connection to ArangoDB as a JSON formatted dictionary using the properties below in the <code>arango_db</code> field. The graph_name should point to the community graph build during the indexing phase.</li> </ol>"},{"location":"eri/config/#garag-config","title":"GARAG config","text":"<pre><code>{\n    \"top_k\": int,\n    \"confidence_cutoff\": float,\n    \"vector_db_index_name\": string\n}\n</code></pre>"},{"location":"eri/config/#arangodb-config","title":"ArangoDB config","text":"<pre><code>{\n    \"url\": string,\n    \"username\": string,\n    \"password\": string,\n    \"db_name\": string,\n    \"graph_name\": string\n}\n</code></pre>"},{"location":"eri/config/#graphrag","title":"GraphRAG","text":"<p>To enable GraphRAG as a retrieval process, please paste the following skeleton into the configuration file and follow these steps closely:</p> <pre><code>[GraphRAG]\nconfig = \nllm = \narango_db = \n</code></pre> <ol> <li>Enable custom configuration: To configure the retriever, please provide a JSON formatted dictionary using the properties below in the <code>config</code> field.<ul> <li><code>top_k (int; 0 &lt; x)</code>: The maximum number of hits to retrieve. May be set by an user during a retrieval request.</li> <li><code>confidence_cutoff (int; 0 &lt; x &lt; 100)</code>: A value to specify a minimum similiarity between the query and the retrieved information. May be set by an user during a retrieval request.</li> <li><code>community_degree (int; 0 &lt; x)</code>: The maximum community degree to be used during retrieval. A higher value will incorporate more precise summaries, resulting in a higher node count and longer query time. May be set by an user during a retrieval request.</li> </ul> </li> <li>Set the LLM connection: Please specify the connection to a LLM service as a JSON formatted dictionary using the properties below in the <code>llm</code> field. This LLM is used to gernerate and evaluate partial answers.</li> <li>Set the ArangoDB connection: Please specify the connection to ArangoDB as a JSON formatted dictionary using the properties below in the <code>arango_db</code> field. The graph_name should point to the community graph build during the indexing phase.</li> </ol>"},{"location":"eri/config/#graphrag-config","title":"GraphRAG config","text":"<pre><code>{\n    \"top_k\": int,\n    \"confidence_cutoff\": int,\n    \"community_degree\": int\n}\n</code></pre>"},{"location":"eri/config/#llm-config","title":"LLM config","text":"<pre><code>{\n    \"provider\": \"ollama\" | \"openai\",\n    \"base_url\": string,\n    \"api_key\": string (optional),\n    \"model_name\": string (optional),\n    \"options\": dictionary\n}\n</code></pre>"},{"location":"eri/config/#arangodb-config_1","title":"ArangoDB config","text":"<pre><code>{\n    \"url\": string,\n    \"username\": string,\n    \"password\": string,\n    \"db_name\": string,\n    \"graph_name\": string\n}\n</code></pre>"},{"location":"eri/config/#naivegraphrag","title":"NaiveGraphRAG","text":"<p>To enable NaiveGraphRAG as a retrieval process, please paste the following skeleton into the configuration file and follow these steps closely:</p> <pre><code>[NaiveGraphRAG]\nconfig = \nemb_model = \nelastic_db_url = \n</code></pre> <ol> <li>Enable custom configuration: To configure the retriever, please provide a JSON formatted dictionary using the properties below in the <code>config</code> field.<ul> <li><code>top_k (int; 0 &lt; x)</code>: The maximum number of hits to retrieve. May be set by an user during a retrieval request.</li> <li><code>confidence_cutoff (float; 0 &lt; x &lt; 1)</code>: A value to specify a minimum similiarity between the query and the retrieved information. May be set by an user during a retrieval request.</li> <li><code>vector_db_index_name (string)</code>: The index to be used for searching relevant information.</li> </ul> </li> <li>Specify the embedding model used: Enter the name of the embedding model used to convert requests into vectors into the <code>emb_model</code> field. This has to match the model used during the indexing phase.</li> <li>Set the Elasticsearch connection: Enter the url to the Elasticsearch server into the <code>elastic_db_url</code> field, on which the vector index can be found.</li> </ol>"},{"location":"eri/config/#naivegraphrag-config","title":"NaiveGraphRAG config","text":"<pre><code>{\n    \"top_k\": int,\n    \"confidence_cutoff\": float,\n    \"vector_db_index_name\": string\n}\n</code></pre>"},{"location":"eri/config/#naiverag","title":"NaiveRAG","text":"<p>To enable NaiveRAG as a retrieval process, please paste the following skeleton into the configuration file and follow these steps closely:</p> <pre><code>[NaiveRAG]\nconfig = \nemb_model = \nelastic_db_url = \n</code></pre> <ol> <li>Enable custom configuration: To configure the retriever, please provide a JSON formatted dictionary using the properties below in the <code>config</code> field.<ul> <li><code>top_k (int; 0 &lt; x)</code>: The maximum number of hits to retrieve. May be set by an user during a retrieval request.</li> <li><code>confidence_cutoff (float; 0 &lt; x &lt; 1)</code>: A value to specify a minimum similiarity between the query and the retrieved information. May be set by an user during a retrieval request.</li> <li><code>vector_db_index_name (string)</code>: The index to be used for searching relevant information.</li> </ul> </li> <li>Specify the embedding model used: Enter the name of the embedding model used to convert requests into vectors into the <code>emb_model</code> field. This has to match the model used during the indexing phase.</li> <li>Set the Elasticsearch connection: Enter the url to the Elasticsearch server into the <code>elastic_db_url</code> field, on which the vector index can be found.</li> </ol>"},{"location":"eri/config/#naiverag-config","title":"NaiveRAG config","text":"<pre><code>{\n    \"top_k\": int,\n    \"confidence_cutoff\": float,\n    \"vector_db_index_name\": string\n}\n</code></pre>"},{"location":"eri/config/#vectorgr","title":"VectorGR","text":"<p>To enable VectorGR as a retrieval process, please paste the following skeleton into the configuration file and follow these steps closely:</p> <pre><code>[VectorGR]\nconfig = \nemb_model = \nllm = \nneo4j_db = \n</code></pre> <ol> <li>Enable custom configuration: To configure the retriever, please provide a JSON formatted dictionary using the properties below in the <code>config</code> field.<ul> <li><code>top_k (int; 0 &lt; x)</code>: The maximum number of hits to retrieve. May be set by an user during a retrieval request.</li> <li><code>v_index_name (string)</code>: </li> <li><code>return_properties (List[string])</code>: List of node properties to return.</li> <li><code>filters (Dictionary[string, Any])</code>: Filters for metadata pre-filtering. When performing a similarity search, one may have constraints to apply. May be set by an user during a retrieval request.</li> </ul> </li> <li>Specify the embedding model used: Enter the name of the embedding model used to convert requests into vectors into the <code>emb_model</code> field. This has to match the model used during the indexing phase.</li> <li>Set the LLM connection: Please specify the connection to a LLM service as a JSON formatted dictionary using the properties below in the <code>llm</code> field. This LLM is used to gernerate and evaluate partial answers.</li> <li>Set the Neo4J connection: Please specify the connection to ArangoDB as a JSON formatted dictionary using the properties below in the <code>neo4j_db</code> field. The graph_name should point to the community graph build during the indexing phase.</li> </ol>"},{"location":"eri/config/#vectorgr-config","title":"VectorGR config","text":"<pre><code>{\n    \"top_k\": int,\n    \"v_index_name\": string,\n    \"return_properties\": List[string],\n    \"filters\": Dictionary[string, Any]\n}\n</code></pre>"},{"location":"eri/config/#llm-config_1","title":"LLM config","text":"<pre><code>{\n    \"provider\": \"ollama\" | \"openai\",\n    \"base_url\": string,\n    \"api_key\": string (optional),\n    \"model_name\": string (optional),\n    \"options\": dictionary\n}\n</code></pre>"},{"location":"eri/config/#neo4jdb-config","title":"Neo4JDB config","text":"<pre><code>{\n    \"url\": string,\n    \"db_name\": string,\n    \"password\": string\n}\n</code></pre>"},{"location":"eri/config/#hybridgr","title":"HybridGR","text":"<p>To enable HybridGR as a retrieval process, please paste the following skeleton into the configuration file and follow these steps closely:</p> <pre><code>[HybridGR]\nconfig = \nemb_model = \nllm = \nneo4j_db = \n</code></pre> <ol> <li>Enable custom configuration: To configure the retriever, please provide a JSON formatted dictionary using the properties below in the <code>config</code> field.<ul> <li><code>top_k (int; 0 &lt; x)</code>: The maximum number of hits to retrieve. May be set by an user during a retrieval request.</li> <li><code>v_index_name (string)</code>: </li> <li><code>f_index_name (string)</code>: </li> <li><code>return_properties (List[string])</code>: List of node properties to return.</li> </ul> </li> <li>Specify the embedding model used: Enter the name of the embedding model used to convert requests into vectors into the <code>emb_model</code> field. This has to match the model used during the indexing phase.</li> <li>Set the LLM connection: Please specify the connection to a LLM service as a JSON formatted dictionary using the properties below in the <code>llm</code> field. This LLM is used to gernerate and evaluate partial answers.</li> <li>Set the Neo4J connection: Please specify the connection to ArangoDB as a JSON formatted dictionary using the properties below in the <code>neo4j_db</code> field. The graph_name should point to the community graph build during the indexing phase.</li> </ol>"},{"location":"eri/config/#hybridgr-config","title":"HybridGR config","text":"<pre><code>{\n    \"top_k\": int,\n    \"v_index_name\": string,\n    \"f_index_name\": string,\n    \"return_properties\": List[string]\n}\n</code></pre>"},{"location":"eri/config/#llm-config_2","title":"LLM config","text":"<pre><code>{\n    \"provider\": \"ollama\" | \"openai\",\n    \"base_url\": string,\n    \"api_key\": string (optional),\n    \"model_name\": string (optional),\n    \"options\": dictionary\n}\n</code></pre>"},{"location":"eri/config/#neo4jdb-config_1","title":"Neo4JDB config","text":"<pre><code>{\n    \"url\": string,\n    \"db_name\": string,\n    \"password\": string\n}\n</code></pre>"},{"location":"eri/config/#text2cypher","title":"Text2Cypher","text":"<p>To enable Text2Cypher as a retrieval process, please paste the following skeleton into the configuration file and follow these steps closely:</p> <pre><code>[Text2Cypher]\nconfig = \nllm = \nneo4j_db = \n</code></pre> <ol> <li>Enable custom configuration: To configure the retriever, please provide a JSON formatted dictionary using the properties below in the <code>config</code> field.<ul> <li><code>examples</code>: Optional user input/query pairs for the LLM to use as examples.</li> </ul> </li> <li>Set the LLM connection: Please specify the connection to a LLM service as a JSON formatted dictionary using the properties below in the <code>llm</code> field. This LLM is used to gernerate and evaluate partial answers.</li> <li>Set the Neo4J connection: Please specify the connection to ArangoDB as a JSON formatted dictionary using the properties below in the <code>neo4j_db</code> field. The graph_name should point to the community graph build during the indexing phase.</li> </ol>"},{"location":"eri/config/#text2cypher-config","title":"Text2Cypher config","text":"<pre><code>{\n    \"examples\": List[string]\n}\n</code></pre>"},{"location":"eri/config/#llm-config_3","title":"LLM config","text":"<pre><code>{\n    \"provider\": \"ollama\" | \"openai\",\n    \"base_url\": string,\n    \"api_key\": string (optional),\n    \"model_name\": string (optional),\n    \"options\": dictionary\n}\n</code></pre>"},{"location":"eri/config/#neo4jdb-config_2","title":"Neo4JDB config","text":"<pre><code>{\n    \"url\": string,\n    \"db_name\": string,\n    \"password\": string\n}\n</code></pre>"},{"location":"eri/config/#authorization-methods","title":"Authorization methods","text":""},{"location":"eri/config/#none","title":"NONE","text":"<p>This method is used to allow free access to all users. Everyone can generate a session token without restrictions, which can the be used to retrieve data from the data source.</p>"},{"location":"eri/config/#token","title":"TOKEN","text":"<p>This method retricts access to the data source by requireing an API key, provided as a Bearer token, when generating a session token. Valid API keys are stored in a Postgres database, allowing other services to manage these keys.</p> <p>To use the <code>TOKEN</code> authorization method, a table named <code>API_Key</code> must be created in the Postgres database. This can be done using the following command: <pre><code>CREATE TABLE IF NOT EXISTS API_Key (\n    key varchar PRIMARY KEY, \n    user varchar\n);\n</code></pre></p> <p>This table is created automatialy on startup, if it does not already exist. During authentication, the provided token is compared to all key fields within the <code>API_Key</code> table. If a match is found, access is granted; otherwise, unauthorized access is denied.</p>"},{"location":"eri/config/#specifying-the-postgres-connection","title":"Specifying the postgres connection","text":"<p>When a database is needed, please provide a JSON formatted dictionary using the following properties in the <code>postgres_connection</code> field in the <code>ERI Settings</code> section:</p> <pre><code>{\n    \"username\": string,\n    \"password\": string,\n    \"url\": string,\n    \"database_name\": string\n}\n</code></pre>"},{"location":"insights/","title":"Overview \ud83d\udca1","text":"<p>In Progress</p>"},{"location":"insights/comparison/","title":"Technique Comparison: Strengths and Limitations of RAG Approaches","text":"<p>In Progress</p>"},{"location":"insights/selection/","title":"Practical Selection Guide: When to Use Which RAG Approach","text":"<p>In Progress</p>"},{"location":"references/techniques/","title":"Techniques","text":"<ul> <li> <p> GraphRAG: Unlocking LLM discovery on narrative private data</p> <p>Published February 13, 2024 <p>By Jonathan Larson; Steven Truitt</p> <li> <p> LazyGraphRAG: Setting a new standard for quality and cost</p> <p>Published November 25, 2024 <p>By Darren Edge;  Ha Trinh; Jonathan Larson</p>"},{"location":"stuff/admonitions/","title":"admonitions","text":"<p>Title of the callout</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Collapsible callout:</p> Collapsible callout <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"stuff/code-examples/","title":"code-examples","text":"<p>An example of a codeblock for Python</p> add_numbers.py<pre><code># Function to add two numbers\ndef add_two_numbers(num1, num2):\n    return num1 + num2\n\n# Example usage\nresult = add_two_numbers(5, 3)\nprint('The sum is:', result)\n</code></pre> add_numbers.py<pre><code># Function to add two numbers\ndef add_two_numbers(num1, num2):\n    return num1 + num2\n\n# Example usage\nresult = add_two_numbers(5, 3)\nprint('The sum is:', result)\n</code></pre>"},{"location":"stuff/content-tabs/","title":"content-tabs","text":""},{"location":"stuff/content-tabs/#content-tabs","title":"Content Tabs","text":"<p>This is some examples of content tabs.</p>"},{"location":"stuff/content-tabs/#generic-content","title":"Generic Content","text":"Plain textUnordered listOrdered list <p>This is some plain text</p> <ul> <li>First item</li> <li>Second item</li> <li>Third item</li> </ul> <ol> <li>First item</li> <li>Second item</li> <li>Third item</li> </ol>"},{"location":"stuff/content-tabs/#code-blocks-in-content-tabs","title":"Code Blocks in Content Tabs","text":"PythonJavaScript <pre><code>def main():\n    print(\"Hello world!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>function main() {\n    console.log(\"Hello world!\");\n}\n\nmain();\n</code></pre>"},{"location":"stuff/diagrams/","title":"Diagram Examples","text":""},{"location":"stuff/diagrams/#flowcharts","title":"Flowcharts","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Failure?};\n  B --&gt;|Yes| C[Investigate...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Success!];</code></pre>"},{"location":"stuff/diagrams/#sequence-diagrams","title":"Sequence Diagrams","text":"<pre><code>sequenceDiagram\n  autonumber\n  Server-&gt;&gt;Terminal: Send request\n  loop Health\n      Terminal-&gt;&gt;Terminal: Check for health\n  end\n  Note right of Terminal: System online\n  Terminal--&gt;&gt;Server: Everything is OK\n  Terminal-&gt;&gt;Database: Request customer data\n  Database--&gt;&gt;Terminal: Customer data</code></pre>"},{"location":"techniques/","title":"Overview","text":""},{"location":"techniques/#introduction","title":"Introduction","text":"<p>Retrieval-Augmented Generation (RAG) is revolutionizing the way we combine information retrieval with generative AI. This repository showcases a curated collection of advanced techniques designed to supercharge your RAG systems, enabling them to deliver more accurate, contextually relevant, and comprehensive responses.</p> <p>Our goal is to provide a valuable resource for researchers and practitioners looking to push the boundaries of what's possible with RAG.</p> <p>RAGlib provides a high-level implementation of most common RAG algorithms independently from the database backend.</p> <p> </p> <p> </p>"},{"location":"techniques/GARAG/","title":"GARAG:  Graph-Assisted Retrieval-Augmented Generation","text":""},{"location":"techniques/GARAG/#overview","title":"Overview","text":"<p>GARAG is an advanced Retrieval-Augmented Generation (RAG) system that prioritizes the accuracy and integrity of information. Building upon the Microsoft GraphRAG pipeline, this approach leverages knowledge graphs to extract relevant information. In contrast to Microsoft GraphRAG, GARAG employs a novel filtering mechanism using the knowledge graph, which first filters out irrelevant information before returning extracts from the original document.</p>"},{"location":"techniques/GARAG/#method-details","title":"Method Details","text":"<ol> <li> <p>Document Processing:</p> <ul> <li>Input documents are segmented into manageable chunks for efficient processing.</li> <li>A large language model (LLM) creates graph nodes and their connections for each text chunk.</li> <li>References to the original documents as sources are preserved and cumulated for each node, ensuring contextual information is maintained.</li> </ul> </li> <li> <p>Thematic Summarization:</p> <ul> <li>The Leiden Algorithm is used to cluster the graph into well-connected communities.</li> <li>These communities are further divided into subcommunities recursively to extract more detailed summaries.</li> <li>For each community, a summarization of all relevant information is generated.</li> <li>Original document sources are accumulated within each community, ensuring contextual integrity.</li> <li>Resulting summaries are stored in a vector store as embeddings for efficient retrieval.</li> </ul> </li> <li> <p>Query Process:</p> <ul> <li>The user query is embedded and used to retrieve relevant summaries from the vector store.</li> <li>The resulting vector similarity score of each hit is used as a weight for the respective cumulated sources.</li> <li>Weighted source references are then accumulated over the extracted information.</li> <li>Using the weighted scores, the text of original documents is returned to the user.</li> </ul> </li> </ol>"},{"location":"techniques/GARAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li> <p>Garanteed Information Integrity: By extracting unaltered extracts from original documents, GARAG ensures the integrity of provided text, eliminating potential biases or distortions.</p> </li> <li> <p>Fast query time: The query process relies on embeddings and vector similarity scores, making it optimized for speed.</p> </li> <li> <p>Global Sensmaking: Although GARAG does not provide community summaries directly, the usage of these summaries in the query process enables global sensmaking for filtering information.</p> </li> </ol>"},{"location":"techniques/GNN-RAG/","title":"GNN-RAG","text":"<p>In Progress</p>"},{"location":"techniques/GraphRAG-dataflow/","title":"Indexing Dataflow","text":""},{"location":"techniques/GraphRAG-dataflow/#the-graphrag-knowledge-model","title":"The GraphRAG Knowledge Model","text":"<p>The following entity types are provided. The fields here represent the fields that are text-embedded by default.</p> <ul> <li><code>Document</code> - An input document into the system. These either represent individual rows in a CSV or individual .txt file.</li> <li><code>TextUnit</code> - A chunk of text to analyze. The size of these chunks, their overlap, and whether they adhere to any data boundaries may be configured below. A common use case is to set <code>CHUNK_BY_COLUMNS</code> to <code>id</code> so that there is a 1-to-many relationship between documents and TextUnits instead of a many-to-many.</li> <li><code>Entity</code> - An entity extracted from a TextUnit. These represent people, places, events, or some other entity-model that you provide.</li> <li><code>Relationship</code> - A relationship between two entities.</li> <li><code>Covariate</code> - Extracted claim information, which contains statements about entities which may be time-bound.</li> <li><code>Community</code> - Once the graph of entities and relationships is built, we perform hierarchical community detection on them to create a clustering structure.</li> <li><code>Community Report</code> - The contents of each community are summarized into a generated report, useful for human reading and downstream search.</li> </ul>"},{"location":"techniques/GraphRAG-dataflow/#the-default-configuration-workflow","title":"The Default Configuration Workflow","text":"<p>Let's take a look at how the default-configuration workflow transforms text documents into the GraphRAG Knowledge Model. This page gives a general overview of the major steps in this process. To fully configure this workflow, check out the configuration documentation.</p> <pre><code>---\ntitle: Dataflow Overview\n---\nflowchart TB\n    subgraph phase1[Phase 1: Compose TextUnits]\n    documents[Documents] --&gt; chunk[Chunk]\n    chunk --&gt; textUnits[Text Units]\n    end\n    subgraph phase2[Phase 2: Graph Extraction]\n    textUnits --&gt; graph_extract[Entity &amp; Relationship Extraction]\n    graph_extract --&gt; graph_summarize[Entity &amp; Relationship Summarization]\n    graph_summarize --&gt; claim_extraction[Claim Extraction]\n    claim_extraction --&gt; graph_outputs[Graph Tables]\n    end\n    subgraph phase3[Phase 3: Graph Augmentation]\n    graph_outputs --&gt; community_detect[Community Detection]\n    community_detect --&gt; community_outputs[Communities Table]\n    end\n    subgraph phase4[Phase 4: Community Summarization]\n    community_outputs --&gt; summarized_communities[Community Summarization]\n    summarized_communities --&gt; community_report_outputs[Community Reports Table]\n    end\n    subgraph phase5[Phase 5: Document Processing]\n    documents --&gt; link_to_text_units[Link to TextUnits]\n    textUnits --&gt; link_to_text_units\n    link_to_text_units --&gt; document_outputs[Documents Table]\n    end\n    subgraph phase6[Phase 6: Network Visualization]\n    graph_outputs --&gt; graph_embed[Graph Embedding]\n    graph_embed --&gt; umap_entities[Umap Entities]\n    umap_entities --&gt; combine_nodes[Final Entities]\n    end\n    subgraph phase7[Phase 7: Text Embeddings]\n    textUnits --&gt; text_embed[Text Embedding]\n    graph_outputs --&gt; description_embed[Description Embedding]\n    community_report_outputs --&gt; content_embed[Content Embedding]\n    end</code></pre>"},{"location":"techniques/GraphRAG-dataflow/#phase-1-compose-textunits","title":"Phase 1: Compose TextUnits","text":"<p>The first phase of the default-configuration workflow is to transform input documents into TextUnits. A TextUnit is a chunk of text that is used for our graph extraction techniques. They are also used as source-references by extracted knowledge items in order to empower breadcrumbs and provenance by concepts back to their original source text.</p> <p>The chunk size (counted in tokens), is user-configurable. By default this is set to 300 tokens, although we've had positive experience with 1200-token chunks using a single \"glean\" step. (A \"glean\" step is a follow-on extraction). Larger chunks result in lower-fidelity output and less meaningful reference texts; however, using larger chunks can result in much faster processing time.</p> <p>The group-by configuration is also user-configurable. By default, we align our chunks to document boundaries, meaning that there is a strict 1-to-many relationship between Documents and TextUnits. In rare cases, this can be turned into a many-to-many relationship. This is useful when the documents are very short and we need several of them to compose a meaningful analysis unit (e.g. Tweets or a chat log)</p> <pre><code>---\ntitle: Documents into Text Chunks\n---\nflowchart LR\n    doc1[Document 1] --&gt; tu1[TextUnit 1]\n    doc1 --&gt; tu2[TextUnit 2]\n    doc2[Document 2] --&gt; tu3[TextUnit 3]\n    doc2 --&gt; tu4[TextUnit 4]\n</code></pre>"},{"location":"techniques/GraphRAG-dataflow/#phase-2-graph-extraction","title":"Phase 2: Graph Extraction","text":"<p>In this phase, we analyze each text unit and extract our graph primitives: Entities, Relationships, and Claims. Entities and Relationships are extracted at once in our entity_extract verb, and claims are extracted in our claim_extract verb. Results are then combined and passed into following phases of the pipeline.</p> <pre><code>---\ntitle: Graph Extraction\n---\nflowchart LR\n    tu[TextUnit] --&gt; ge[Graph Extraction] --&gt; gs[Graph Summarization]\n    tu --&gt; ce[Claim Extraction]</code></pre>"},{"location":"techniques/GraphRAG-dataflow/#entity-relationship-extraction","title":"Entity &amp; Relationship Extraction","text":"<p>In this first step of graph extraction, we process each text-unit in order to extract entities and relationships out of the raw text using the LLM. The output of this step is a subgraph-per-TextUnit containing a list of entities with a title, type, and description, and a list of relationships with a source, target, and description.</p> <p>These subgraphs are merged together - any entities with the same title and type are merged by creating an array of their descriptions. Similarly, any relationships with the same source and target are merged by creating an array of their descriptions.</p>"},{"location":"techniques/GraphRAG-dataflow/#entity-relationship-summarization","title":"Entity &amp; Relationship Summarization","text":"<p>Now that we have a graph of entities and relationships, each with a list of descriptions, we can summarize these lists into a single description per entity and relationship. This is done by asking the LLM for a short summary that captures all of the distinct information from each description. This allows all of our entities and relationships to have a single concise description.</p>"},{"location":"techniques/GraphRAG-dataflow/#claim-extraction-optional","title":"Claim Extraction (optional)","text":"<p>Finally, as an independent workflow, we extract claims from the source TextUnits. These claims represent positive factual statements with an evaluated status and time-bounds. These get exported as a primary artifact called Covariates.</p> <p>Note: claim extraction is optional and turned off by default. This is because claim extraction generally requires prompt tuning to be useful.</p>"},{"location":"techniques/GraphRAG-dataflow/#phase-3-graph-augmentation","title":"Phase 3: Graph Augmentation","text":"<p>Now that we have a usable graph of entities and relationships, we want to understand their community structure. These give us explicit ways of understanding the topological structure of our graph.</p> <pre><code>---\ntitle: Graph Augmentation\n---\nflowchart LR\n    cd[Leiden Hierarchical Community Detection] --&gt; ag[Graph Tables]</code></pre>"},{"location":"techniques/GraphRAG-dataflow/#community-detection","title":"Community Detection","text":"<p>In this step, we generate a hierarchy of entity communities using the Hierarchical Leiden Algorithm. This method will apply a recursive community-clustering to our graph until we reach a community-size threshold. This will allow us to understand the community structure of our graph and provide a way to navigate and summarize the graph at different levels of granularity.</p>"},{"location":"techniques/GraphRAG-dataflow/#graph-tables","title":"Graph Tables","text":"<p>Once our graph augmentation steps are complete, the final Entities, Relationships, and Communities tables are exported.</p>"},{"location":"techniques/GraphRAG-dataflow/#phase-4-community-summarization","title":"Phase 4: Community Summarization","text":"<pre><code>---\ntitle: Community Summarization\n---\nflowchart LR\n    sc[Generate Community Reports] --&gt; ss[Summarize Community Reports] --&gt; co[Community Reports Table]</code></pre> <p>At this point, we have a functional graph of entities and relationships and a hierarchy of communities for the entities.</p> <p>Now we want to build on the communities data and generate reports for each community. This gives us a high-level understanding of the graph at several points of graph granularity. For example, if community A is the top-level community, we'll get a report about the entire graph. If the community is lower-level, we'll get a report about a local cluster.</p>"},{"location":"techniques/GraphRAG-dataflow/#generate-community-reports","title":"Generate Community Reports","text":"<p>In this step, we generate a summary of each community using the LLM. This will allow us to understand the distinct information contained within each community and provide a scoped understanding of the graph, from either a high-level or a low-level perspective. These reports contain an executive overview and reference the key entities, relationships, and claims within the community sub-structure.</p>"},{"location":"techniques/GraphRAG-dataflow/#summarize-community-reports","title":"Summarize Community Reports","text":"<p>In this step, each community report is then summarized via the LLM for shorthand use.</p>"},{"location":"techniques/GraphRAG-dataflow/#community-reports-table","title":"Community Reports Table","text":"<p>At this point, some bookkeeping work is performed and we export the Community Reports tables.</p>"},{"location":"techniques/GraphRAG-dataflow/#phase-5-document-processing","title":"Phase 5: Document Processing","text":"<p>In this phase of the workflow, we create the Documents table for the knowledge model.</p> <pre><code>---\ntitle: Document Processing\n---\nflowchart LR\n    aug[Augment] --&gt; dp[Link to TextUnits] --&gt; dg[Documents Table]</code></pre>"},{"location":"techniques/GraphRAG-dataflow/#augment-with-columns-csv-only","title":"Augment with Columns (CSV Only)","text":"<p>If the workflow is operating on CSV data, you may configure your workflow to add additional fields to Documents output. These fields should exist on the incoming CSV tables. Details about configuring this can be found in the configuration documentation.</p>"},{"location":"techniques/GraphRAG-dataflow/#link-to-textunits","title":"Link to TextUnits","text":"<p>In this step, we link each document to the text-units that were created in the first phase. This allows us to understand which documents are related to which text-units and vice-versa.</p>"},{"location":"techniques/GraphRAG-dataflow/#documents-table","title":"Documents Table","text":"<p>At this point, we can export the Documents table into the knowledge Model.</p>"},{"location":"techniques/GraphRAG-dataflow/#phase-6-network-visualization-optional","title":"Phase 6: Network Visualization (optional)","text":"<p>In this phase of the workflow, we perform some steps to support network visualization of our high-dimensional vector spaces within our existing graphs. At this point there are two logical graphs at play: the Entity-Relationship graph and the Document graph.</p> <pre><code>---\ntitle: Network Visualization Workflows\n---\nflowchart LR\n    ag[Graph Table] --&gt; ge[Node2Vec Graph Embedding] --&gt; ne[Umap Entities] --&gt; ng[Entities Table]</code></pre>"},{"location":"techniques/GraphRAG-dataflow/#graph-embedding","title":"Graph Embedding","text":"<p>In this step, we generate a vector representation of our graph using the Node2Vec algorithm. This will allow us to understand the implicit structure of our graph and provide an additional vector-space in which to search for related concepts during our query phase.</p>"},{"location":"techniques/GraphRAG-dataflow/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>For each of the logical graphs, we perform a UMAP dimensionality reduction to generate a 2D representation of the graph. This will allow us to visualize the graph in a 2D space and understand the relationships between the nodes in the graph. The UMAP embeddings are reduced to two dimensions as x/y coordinates.</p>"},{"location":"techniques/GraphRAG-dataflow/#phase-7-text-embedding","title":"Phase 7: Text Embedding","text":"<p>For all artifacts that require downstream vector search, we generate text embeddings as a final step. These embeddings are written directly to a configured vector store. By default we embed entity descriptions, text unit text, and community report text.</p> <pre><code>---\ntitle: Text Embedding Workflows\n---\nflowchart LR\n    textUnits[Text Units] --&gt; text_embed[Text Embedding]\n    graph_outputs[Graph Tables] --&gt; description_embed[Description Embedding]\n    community_report_outputs[Community Reports] --&gt; content_embed[Content Embedding]</code></pre>"},{"location":"techniques/GraphRAG-overview/","title":"Microsoft GraphRAG:  Enhancing Retrieval-Augmented Generation with Knowledge Graphs","text":""},{"location":"techniques/GraphRAG-overview/#overview","title":"Overview","text":"<p>Microsoft GraphRAG is an advanced Retrieval-Augmented Generation (RAG) system that integrates knowledge graphs to improve the performance of large language models (LLMs). Developed by Microsoft Research, GraphRAG addresses limitations in traditional RAG approaches by using LLM-generated knowledge graphs to enhance document analysis and improve response quality.</p>"},{"location":"techniques/GraphRAG-overview/#motivation","title":"Motivation","text":"<p>Traditional RAG systems often struggle with complex queries that require synthesizing information from disparate sources. GraphRAG aims to: Connect related information across datasets. Enhance understanding of semantic concepts. Improve performance on global sensemaking tasks.</p>"},{"location":"techniques/GraphRAG-overview/#key-components","title":"Key Components","text":"<p>Knowledge Graph Generation: Constructs graphs with entities as nodes and relationships as edges. Community Detection: Identifies clusters of related entities within the graph. Summarization: Generates summaries for each community to provide context for LLMs. Query Processing: Uses these summaries to enhance the LLM's ability to answer complex questions.</p>"},{"location":"techniques/GraphRAG-overview/#method-details","title":"Method Details","text":""},{"location":"techniques/GraphRAG-overview/#indexing-stage","title":"Indexing Stage","text":"<p>Text Chunking: Splits source texts into manageable chunks. Element Extraction: Uses LLMs to identify entities and relationships. Graph Construction: Builds a graph from the extracted elements. Community Detection: Applies algorithms like Leiden to find communities. Community Summarization: Creates summaries for each community.</p>"},{"location":"techniques/GraphRAG-overview/#query-stage","title":"Query Stage","text":"<p>Local Answer Generation: Uses community summaries to generate preliminary answers. Global Answer Synthesis: Combines local answers to form a comprehensive response.</p>"},{"location":"techniques/GraphRAG-overview/#benefits-of-graphrag","title":"Benefits of GraphRAG","text":"<p>GraphRAG is a powerful tool that addresses some of the key limitations of the baseline RAG model. Unlike the standard RAG model, GraphRAG excels at identifying connections between disparate pieces of information and drawing insights from them. This makes it an ideal choice for users who need to extract insights from large data collections or documents that are difficult to summarize. By leveraging its advanced graph-based architecture, GraphRAG is able to provide a holistic understanding of complex semantic concepts, making it an invaluable tool for anyone who needs to find information quickly and accurately. Whether you're a researcher, analyst, or just someone who needs to stay informed, GraphRAG can help you connect the dots and uncover new insights.</p>"},{"location":"techniques/GraphRAG-overview/#conclusion","title":"Conclusion","text":"<p>Microsoft GraphRAG represents a significant step forward in retrieval-augmented generation, particularly for tasks requiring a global understanding of datasets. By incorporating knowledge graphs, it offers improved performance, making it ideal for complex information retrieval and analysis.</p> <p>For those experienced with basic RAG systems, GraphRAG offers an opportunity to explore more sophisticated solutions, although it may not be necessary for all use cases. Retrieval Augmented Generation (RAG) is often performed by chunking long texts, creating a text embedding for each chunk, and retrieving chunks for including in the LLM generation context based on a similarity search against the query. This approach works well in many scenarios, and at compelling speed and cost trade-offs, but doesn't always cope well in scenarios where a detailed understanding of the text is required.</p>"},{"location":"techniques/GraphRAG/","title":"GraphRAG:  Graph-Enhanced Retrieval-Augmented Generation","text":""},{"location":"techniques/GraphRAG/#overview","title":"Overview","text":"<p>GraphRAG is an advanced question-answering system that combines the power of graph-based knowledge representation with retrieval-augmented generation. It processes input documents to create a rich knowledge graph, which is then used to enhance the retrieval and generation of answers to user queries. The system leverages natural language processing, machine learning, and graph theory to provide more accurate and contextually relevant responses.</p>"},{"location":"techniques/GraphRAG/#motivation","title":"Motivation","text":"<p>Traditional retrieval-augmented generation systems often struggle with maintaining context over long documents and making connections between related pieces of information. GraphRAG addresses these limitations by:</p> <ol> <li>Representing knowledge as an interconnected graph, allowing for better preservation of relationships between concepts.</li> <li>Enabling more intelligent traversal of information during the query process.</li> <li>Providing a visual representation of how information is connected and accessed during the answering process.</li> </ol>"},{"location":"techniques/GraphRAG/#key-components","title":"Key Components","text":"<ol> <li> <p>DocumentProcessor: Handles the initial processing of input documents, creating text chunks and embeddings.</p> </li> <li> <p>KnowledgeGraph: Constructs a graph representation of the processed documents, where nodes represent text chunks and edges represent relationships between them.</p> </li> <li> <p>QueryEngine: Manages the process of answering user queries by leveraging the knowledge graph and vector store.</p> </li> </ol>"},{"location":"techniques/GraphRAG/#method-details","title":"Method Details","text":"<ol> <li> <p>Document Processing:</p> <ul> <li>Input documents are split into manageable chunks.</li> <li>Each chunk is embedded using a language model.</li> <li>A vector store is created from these embeddings for efficient similarity search.</li> </ul> </li> <li> <p>Knowledge Graph Construction:</p> <ul> <li>Graph nodes are created for each text chunk.</li> <li>Concepts are extracted from each chunk using a combination of NLP techniques and language models.</li> <li>Extracted concepts are lemmatized to improve matching.</li> <li>Edges are added between nodes based on semantic similarity and shared concepts.</li> <li>Edge weights are calculated to represent the strength of relationships.</li> </ul> </li> <li> <p>Query Processing:</p> <ul> <li>The user query is embedded and used to retrieve relevant documents from the vector store.</li> <li>A priority queue is initialized with the nodes corresponding to the most relevant documents.</li> <li>The system employs a Dijkstra-like algorithm to traverse the knowledge graph:<ul> <li>Nodes are explored in order of their priority (strength of connection to the query).</li> <li>For each explored node:</li> <li>Its content is added to the context.</li> <li>The system checks if the current context provides a complete answer.</li> <li>If the answer is incomplete:<ul> <li>The node's concepts are processed and added to a set of visited concepts.</li> <li>Neighboring nodes are explored, with their priorities updated based on edge weights.</li> <li>Nodes are added to the priority queue if a stronger connection is found.</li> </ul> </li> </ul> </li> <li>This process continues until a complete answer is found or the priority queue is exhausted.</li> <li>If no complete answer is found after traversing the graph, the system generates a final answer using the accumulated context and a large language model.</li> </ul> </li> </ol>"},{"location":"techniques/GraphRAG/#benefits-of-this-approach","title":"Benefits of This Approach","text":"<ol> <li> <p>Improved Context Awareness: By representing knowledge as a graph, the system can maintain better context and make connections across different parts of the input documents.</p> </li> <li> <p>Enhanced Retrieval: The graph structure allows for more intelligent retrieval of information, going beyond simple keyword matching.</p> </li> <li> <p>Explainable Results: The visualization of the graph and traversal path provides insight into how the system arrived at its answer, improving transparency and trust.</p> </li> <li> <p>Flexible Knowledge Representation: The graph structure can easily incorporate new information and relationships as they become available.</p> </li> <li> <p>Efficient Information Traversal: The weighted edges in the graph allow the system to prioritize the most relevant information pathways when answering queries.</p> </li> </ol>"},{"location":"techniques/GraphRAG/#conclusion","title":"Conclusion","text":"<p>GraphRAG represents a significant advancement in retrieval-augmented generation systems. By incorporating a graph-based knowledge representation and intelligent traversal mechanisms, it offers improved context awareness, more accurate retrieval, and enhanced explainability. The system's ability to visualize its decision-making process provides valuable insights into its operation, making it a powerful tool for both end-users and developers. As natural language processing and graph-based AI continue to evolve, systems like GraphRAG pave the way for more sophisticated and capable question-answering technologies.</p>"},{"location":"techniques/LightRAG/","title":"LightRAG","text":""},{"location":"techniques/NaiveGraphRAG/","title":"Naive GraphRAG","text":""},{"location":"techniques/NaiveGraphRAG/#overview","title":"Overview","text":"<p>The Naive GraphRAG variant leverages a custom implementation of the Microsoft GraphRAG Approach, incorporating key principles from VectorRAG to optimize query performance in this graph-based system.</p>"},{"location":"techniques/NaiveGraphRAG/#method-details","title":"Method Details","text":"<ol> <li> <p>Document Processing:</p> <ul> <li>Input documents are segmented into manageable chunks for efficient processing.</li> <li>A large language model (LLM) creates graph nodes and their connections for each text chunk.</li> </ul> </li> <li> <p>Thematic Summarization:</p> <ul> <li>The Leiden Algorithm is used to cluster the graph into well-connected communities.</li> <li>These communities are further divided into subcommunities recursively to extract more detailed summaries.</li> <li>For each community, a summarization of all relevant information is generated.</li> <li>Resulting summaries are stored in a vector store as embeddings for efficient retrieval.</li> </ul> </li> <li> <p>Query Process:</p> <ul> <li>The user query is embedded to retrieve relevant summaries from the vector store.</li> <li>The embedded query is used to filter and retrieve relevant summaries, which are then returned to the user.</li> </ul> </li> </ol>"},{"location":"techniques/NaiveGraphRAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li> <p>Fast query time: The query process relies on embeddings and vector similarity scores, making it optimized for speed.</p> </li> <li> <p>Global Sensmaking: The usage of community summaries in the query process enables global sensmaking for filtering information.</p> </li> </ol>"},{"location":"techniques/PathRAG/","title":"PathRAG","text":""},{"location":"techniques/T-RAG/","title":"T-RAG","text":""},{"location":"techniques/simpleRAG/","title":"Vector RAG","text":""},{"location":"techniques/simpleRAG/#overview","title":"Overview","text":"<p>Vector RAG is a lightweight Retrieval-Augmented Generation (RAG) system that leverages embedding-based retrieval to efficiently retrieve relevant information. This approach, also known as naive RAG, has gained popularity due to its simplicity and efficiency.</p>"},{"location":"techniques/simpleRAG/#method-details","title":"Method Details","text":"<ol> <li> <p>Document indexing:</p> <ul> <li>Input focuments are segmented into managable chunks for efficient processing.</li> <li>These chunks are then storead in a vectore store as embeddings for efficient retrieval.</li> </ul> </li> <li> <p>Query Process:</p> <ul> <li>The user query is embedded to retrieve relevant summaries from the vector store.</li> <li>The embedded query is used to filter and retrieve relevant summaries, which are then returned to the user.</li> </ul> </li> </ol>"},{"location":"techniques/simpleRAG/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ol> <li> <p>Very efficient indexing and query: By relying solely on embeddings for retrieval, this system enjoys fast indexing and query processing times.</p> </li> <li> <p>Great Baseline: The simplicity of this algorithm makes it an ideal baseline for comparing various retrieval methods, providing a reliable starting point for evaluating the effectiveness of more complex approaches.</p> </li> </ol>"}]}